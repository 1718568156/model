import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
import torch.nn.functional as F
from tqdm import tqdm 
import time
import matplotlib
matplotlib.use('Agg') 
import matplotlib.pyplot as plt
from torch.optim.lr_scheduler import CosineAnnealingLR


print("--- Step 1: åº“å¯¼å…¥æˆåŠŸ ---")



transform = transforms.Compose(
    [
    transforms.RandomHorizontalFlip(), # 50%çš„æ¦‚ç‡æ°´å¹³ç¿»è½¬å›¾ç‰‡
    transforms.RandomCrop(32, padding=4), #å¡«å……åŠ è£å‰ª
    transforms.ToTensor(),#å½’ä¸€åŒ–
   transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])#è®­ç»ƒé›†

transform_test = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
])#æµ‹è¯•é›†

# å®šä¹‰æ¯ä¸ªæ‰¹æ¬¡åŠ è½½çš„å›¾ç‰‡æ•°é‡
batch_size = 128
#è½®æ¬¡
epochs = 40

trainset = torchvision.datasets.CIFAR10(root='./data', train=True,
                                        download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,
                                          shuffle=True, num_workers=8)#shuffleæ‰“ä¹±æ•°æ®ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ

# å‡†å¤‡æµ‹è¯•æ•°æ®é›†ï¼ˆè¿‡ç¨‹åŒä¸Šï¼Œåªæ˜¯ train=Falseï¼‰
testset = torchvision.datasets.CIFAR10(root='./data', train=False,
                                       download=True, transform=transform_test)
testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,
                                         shuffle=False, num_workers=8)

# å®šä¹‰10ä¸ªç±»åˆ«çš„åç§°ï¼Œæ–¹ä¾¿åç»­æŸ¥çœ‹
classes = ('plane', 'car', 'bird', 'cat', 'deer', 
           'dog', 'frog', 'horse', 'ship', 'truck')

print("--- Step 2: æ•°æ®é›†å‡†å¤‡å®Œæ¯• ---")

#æ”¹ç‰ˆå®šä¹‰
class net(nn.Module):
    def __init__(self):
         super().__init__()
         self.conv1=nn.Sequential(
         nn.Conv2d(in_channels=3,out_channels=32,kernel_size=3,padding=1),
         nn.BatchNorm2d(32),
         nn.ReLU(inplace=True),#inplaceä¼šç ´åå½“å‰å±‚çš„æ•°æ®ï¼Œä»è€ŒèŠ‚çœå†…å­˜
         nn.Conv2d(in_channels=32,out_channels=64,kernel_size=3,padding=1),
         nn.BatchNorm2d(64),
         nn.ReLU(inplace=True),
         # æœ€å¤§æ± åŒ–å±‚ï¼Œå°†å°ºå¯¸å‡åŠ
         nn.MaxPool2d(kernel_size=2, stride=2)


         )
         self.conv_block2 = nn.Sequential(
         # é€šé“æ•°ä»64å¢åŠ åˆ°128
         nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),
         nn.BatchNorm2d(128),
         nn.ReLU(inplace=True),
            
         # ä¿æŒ128é€šé“ï¼Œè¿›ä¸€æ­¥æå–ç‰¹å¾
         nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),
         nn.BatchNorm2d(128),
         nn.ReLU(inplace=True),
            
         # å†æ¬¡æœ€å¤§æ± åŒ–ï¼Œå°ºå¯¸å‡åŠ
         nn.MaxPool2d(kernel_size=2, stride=2) # å°ºå¯¸: 16x16 -> 8x8
        )
         self.classifier = nn.Sequential(
         nn.Flatten(),#é“ºå¹³
            # å…¨è¿æ¥å±‚ã€‚è¾“å…¥ç»´åº¦æ ¹æ®ä¸Šé¢è®¡ç®—å¾—å‡ºï¼š128ä¸ªé€šé“ * 8x8çš„ç‰¹å¾å›¾
         nn.Linear(128 * 8 * 8, 1024),
         nn.ReLU(inplace=True),
            
            # åŠ å…¥Dropoutå±‚ï¼Œä»¥50%çš„æ¦‚ç‡éšæœºä¸¢å¼ƒç¥ç»å…ƒï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
         nn.Dropout(0.5),
            
         nn.Linear(1024, 512),
         nn.ReLU(inplace=True),
         nn.Dropout(0.5),
            
            # æœ€ç»ˆè¾“å‡ºå±‚ï¼Œ10ä¸ªç±»åˆ«
         nn.Linear(512, 10)
        )
    def forward(self, x):
        # å®šä¹‰æ•°æ®æµï¼šå…ˆé€šè¿‡ä¸¤ä¸ªå·ç§¯å—ï¼Œå†é€šè¿‡åˆ†ç±»å™¨
        x = self.conv1(x)
        x = self.conv_block2(x)
        x = self.classifier(x)
        return x
        
net=net()
# å°†ç½‘ç»œé€åˆ°GPUä¸Šï¼ˆå¦‚æœGPUå¯ç”¨ï¼‰
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
net.to(device)

print(f"--- Step 3: æ¨¡å‹å®šä¹‰å®Œæ¯•ï¼Œå°†åœ¨ {device} ä¸Šè¿è¡Œ ---")

# æŸå¤±å‡½æ•°ï¼šä½¿ç”¨äº¤å‰ç†µæŸå¤±
criterion = nn.CrossEntropyLoss()

# ä¼˜åŒ–å™¨ï¼šä½¿ç”¨å¸¦åŠ¨é‡çš„éšæœºæ¢¯åº¦ä¸‹é™(SGD)
optimizer = optim.Adam(net.parameters(), lr=0.001)#momentumæƒ¯æ€§å˜é‡

#å­¦ä¹ ç‡è°ƒåº¦å™¨
scheduler = CosineAnnealingLR(optimizer, T_max=epochs)

print("--- Step 4: æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨å®šä¹‰å®Œæ¯• ---")

#æ–°åŠ ä¸€ä¸ªæ¨¡å— è¯„ä¼°æ¨¡å—

def evaluate(net , dataloader , device):#è¯„ä¼°å‡†ç¡®ç‡
    net.eval() # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼ å½“åˆ‡æ¢åˆ°è¿™ä¸€å±‚æ—¶æŸäº›ç½‘ç»œå±‚ä¼šå…³é—­åŠŸèƒ½
    correct = 0#æ­£ç¡®å›¾ç‰‡
    total = 0#æ‰€æœ‰å›¾ç‰‡
    with torch.no_grad(): # è¯„ä¼°æœŸé—´ä¸éœ€è¦è®¡ç®—æ¢¯åº¦ï¼Œä¸éœ€è¦ä¿ç•™ä¸€äº›æ•°æ®ç”¨äºåå‘ä¼ æ’­
        for data in dataloader:
            images, labels = data[0].to(device), data[1].to(device)
            outputs = net(images)#æ¥æ¬¡å‰å‘ä¼ æ’­
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    accuracy = 100 * correct / total
    return accuracy


print("--- Step 5: å¼€å§‹è®­ç»ƒ ---")
total_start_time = time.time() # è®°å½•æ€»çš„å¼€å§‹æ—¶é—´
best_accuracy = 0.0
PATH = './cifar_best_model.pth'
history = {'train_loss': [], 'test_accuracy': []}#ä¸¤ä¸ªå­—å…¸

for epoch in range(epochs): 
    net.train() 
    running_loss = 0.0
    progress_bar = tqdm(enumerate(trainloader, 0), total=len(trainloader), desc=f"Epoch {epoch + 1}/{epochs}")

    for i, data in progress_bar:
        # 1. è·å–ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®ï¼Œå¹¶é€åˆ°GPU
        # data æ˜¯ä¸€ä¸ªåŒ…å« [inputs, labels] çš„åˆ—è¡¨
        inputs, labels = data[0].to(device), data[1].to(device)

        # 2. æ¢¯åº¦æ¸…é›¶
        optimizer.zero_grad()#é˜²æ­¢ç´¯åŠ 

        # 3. æ ¸å¿ƒä¸‰æ­¥ï¼šå‰å‘ä¼ æ’­ -> è®¡ç®—æŸå¤± -> åå‘ä¼ æ’­
        outputs = net(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        
        # 4. æ›´æ–°æƒé‡
        optimizer.step()

        # 5. æ‰“å°è®­ç»ƒè¿‡ç¨‹ä¸­çš„ä¿¡æ¯
        running_loss += loss.item()
       
    #è¯„ä¼°é˜¶æ®µ
    print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / len(trainloader)}')
    scheduler.step()#æ›´æ–°å­¦ä¹ ç‡
    test_accuracy = evaluate(net, testloader, device)
    history['train_loss'].append(running_loss / len(trainloader))
    history['test_accuracy'].append(test_accuracy)
    print(f"\nEpoch {epoch+1} Summary | Train Loss: {running_loss / len(trainloader):.4f} | Test Accuracy: {test_accuracy:.2f}%")
    if test_accuracy > best_accuracy:
        best_accuracy = test_accuracy
        print(f"ğŸ‰ New best accuracy! Saving model to '{PATH}'")
        torch.save(net.state_dict(), PATH)


    


total_end_time = time.time()
total_duration = total_end_time - total_start_time
print(f'è®­ç»ƒå…¨éƒ¨å®Œæˆï¼æ€»è€—æ—¶: {total_duration:.2f} ç§’')
print("--- è®­ç»ƒç»“æŸ! ---")


#ç”»å›¾
print("--- Step 7: ç»˜åˆ¶è®­ç»ƒæ›²çº¿ ---")

plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(history['train_loss'], label='Training Loss')
plt.title('Training Loss per Epoch')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.grid(True)
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history['test_accuracy'], label='Test Accuracy', color='orange')
plt.title('Test Accuracy per Epoch')
plt.xlabel('Epoch')
plt.ylabel('Accuracy (%)')
plt.grid(True)
plt.legend()

plt.tight_layout()
plt.savefig('training_curves.png')
print("--- è®­ç»ƒæ›²çº¿å›¾å·²ä¿å­˜åˆ° training_curves.png ---")
plt.show()